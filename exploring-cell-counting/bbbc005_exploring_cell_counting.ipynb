{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bbbc005-exploring-cell-counting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZRRAf8ZOqKT",
        "colab_type": "code",
        "outputId": "4e3ba616-6348-49c3-af18-7925a7ba6396",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# For providing authentication to google drive directory.\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct80Pc24Ouqu",
        "colab_type": "code",
        "outputId": "dd8a4187-3f62-4d0a-9424-46e98a94ad86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Change to working directory where project files are present.\n",
        "%cd /content/gdrive/My\\ Drive/colab_notebooks/exploring-cell-counting"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/colab_notebooks/exploring-cell-counting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3L9iKZtOuth",
        "colab_type": "code",
        "outputId": "b3f033d4-1ad4-420c-8cda-ce5b601bfaac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bbbc005_cells\t\t\t       cell_counting_nac.hdf5\n",
            "bbbc005-exploring-cell-counting.ipynb  cells\n",
            "cell_counting_demo_run.hdf5\t       exploring-cell-counting.ipynb\n",
            "cell_counting_nac_fcrn_bbbc005.hdf5    generator.py\n",
            "cell_counting_nac_fcrn.hdf5\t       __pycache__\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QuopmxZOuxI",
        "colab_type": "code",
        "outputId": "4ab12e04-2f25-4013-bfe5-c54deb6936dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# import statements for constructing the model design.\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import (\n",
        "    Input,\n",
        "    Activation,\n",
        "    concatenate,\n",
        "    add,\n",
        "    merge,\n",
        "    Dropout,\n",
        "    Reshape,\n",
        "    Permute,\n",
        "    Dense,\n",
        "    UpSampling2D,\n",
        "    Flatten\n",
        "    )\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers.convolutional import (\n",
        "    Convolution2D)\n",
        "from keras.layers.pooling import (\n",
        "    MaxPooling2D,\n",
        "    AveragePooling2D\n",
        "    )\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras.layers import *\n",
        "from keras.initializers import *\n",
        "from keras.models import *\n",
        "from keras import initializers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJzbjkxpQCoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code is referenced from: [kgrm](https://github.com/kgrm)\n",
        "class NALU(Layer):\n",
        "    def __init__(self, units, MW_initializer='glorot_uniform',\n",
        "                 G_initializer='glorot_uniform', mode=\"NALU\",\n",
        "                 **kwargs):\n",
        "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
        "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
        "        super(NALU, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.mode = mode\n",
        "        self.MW_initializer = initializers.get(MW_initializer)\n",
        "        self.G_initializer = initializers.get(G_initializer)\n",
        "        self.input_spec = InputSpec(min_ndim=2)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 2\n",
        "        input_dim = input_shape[-1]\n",
        "\n",
        "        self.W_hat = self.add_weight(shape=(input_dim, self.units),\n",
        "                                     initializer=self.MW_initializer,\n",
        "                                     name='W_hat')\n",
        "        self.M_hat = self.add_weight(shape=(input_dim, self.units),\n",
        "                                     initializer=self.MW_initializer,\n",
        "                                     name='M_hat')\n",
        "        if self.mode == \"NALU\":\n",
        "            self.G = self.add_weight(shape=(input_dim, self.units),\n",
        "                                     initializer=self.G_initializer,\n",
        "                                     name='G')\n",
        "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        W = K.tanh(self.W_hat) * K.sigmoid(self.M_hat)\n",
        "        a = K.dot(inputs, W)\n",
        "        if self.mode == \"NAC\":\n",
        "            output = a\n",
        "        elif self.mode == \"NALU\":\n",
        "            m = K.exp(K.dot(K.log(K.abs(inputs) + 1e-7), W))\n",
        "            g = K.sigmoid(K.dot(K.abs(inputs), self.G))\n",
        "            output = g * a + (1 - g) * m\n",
        "        else:\n",
        "            raise ValueError(\"Valid modes: 'NAC', 'NALU'.\")\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert input_shape and len(input_shape) >= 2\n",
        "        assert input_shape[-1]\n",
        "        output_shape = list(input_shape)\n",
        "        output_shape[-1] = self.units\n",
        "        return tuple(output_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_Ay0rczRQoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# weight decay defined on model's parametric matrices for regularization purpose.\n",
        "weight_decay = 1e-5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns1SifKzRQwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Models that can be trained using this notebook on custom create bbbc-005 dataset.\n",
        "# 1. Regular FCRN model.\n",
        "# 2. FCRN model with NALU/NAC Unit.\n",
        "# 3. Regular U-net Model.\n",
        "# 4. U-net model with NAC/NALU Unit."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0tCnJZeRQuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _conv_bn_relu(nb_filter, row, col, subsample = (1,1)):\n",
        "    def f(input):\n",
        "        conv_a = Convolution2D(nb_filter, row, col, subsample = subsample,\n",
        "                               init = 'orthogonal', \n",
        "                               border_mode='same', bias = False)(input)\n",
        "        norm_a = BatchNormalization()(conv_a)\n",
        "        # 1. This actionvation function can be changed as demonstrated in experiments to create FCRN/U-net variants.\n",
        "        act_a = Activation(activation = 'relu')(norm_a)\n",
        "        return act_a\n",
        "    return f\n",
        "\n",
        "def _conv_bn_lin(nb_filter, row, col, subsample = (1,1)):\n",
        "    def f(input):\n",
        "        conv_a = Convolution2D(nb_filter, row, col, subsample = subsample,\n",
        "                               init = 'orthogonal', \n",
        "                               border_mode='same', bias = False)(input)\n",
        "        norm_a = BatchNormalization()(conv_a)\n",
        "        # 2. For demonstration a linear batch normalization layer created that can also be used.\n",
        "        act_a = Activation(activation = 'linear')(norm_a)\n",
        "        return act_a\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BySiw1UYRQru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _conv_bn_relu_x2(nb_filter, row, col, subsample = (1,1)):\n",
        "    def f(input):\n",
        "        # 1. batch normalization with relu activation, again it can be changed to create different variants of FCRN & U-net.\n",
        "        conv_a = Convolution2D(nb_filter, row, col, subsample = subsample,\n",
        "                               init = 'orthogonal', border_mode = 'same',bias = False,\n",
        "                               W_regularizer = l2(weight_decay),\n",
        "                               b_regularizer = l2(weight_decay))(input)\n",
        "        norm_a = BatchNormalization()(conv_a)\n",
        "        act_a = Activation(activation = 'relu')(norm_a)\n",
        "        conv_b = Convolution2D(nb_filter, row, col, subsample = subsample,\n",
        "                              init = 'orthogonal', border_mode = 'same',bias = False,\n",
        "                              W_regularizer = l2(weight_decay),\n",
        "                              b_regularizer = l2(weight_decay))(act_a)\n",
        "        norm_b = BatchNormalization()(conv_b)\n",
        "        act_b = Activation(activation = 'relu')(norm_b)\n",
        "        return act_a\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY-JzTrlbN_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. FCRN base and FCRN/NALU base model structure decleration.\n",
        "\n",
        "\n",
        "def fcrn_base(input):\n",
        "    # This model contains convolutional operation with 2x batch normalization function.\n",
        "    # One time batch normalization can also opted, if required. Change _conv_bn_relu_x2 to _conv_bn_relu\n",
        "    block1 = _conv_bn_relu_x2(32,3,3)(input)\n",
        "    pool1 = MaxPooling2D(pool_size=(2,2))(block1)\n",
        "    # =========================================================================\n",
        "    block2 = _conv_bn_relu_x2(64,3,3)(pool1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(block2)\n",
        "    # =========================================================================\n",
        "    block3 = _conv_bn_relu_x2(128,3,3)(pool2)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(block3)\n",
        "    # =========================================================================\n",
        "    block4 = _conv_bn_relu(512,3,3)(pool3)\n",
        "    # =========================================================================\n",
        "    up5 = UpSampling2D(size=(2, 2))(block4)\n",
        "    block5 = _conv_bn_relu_x2(128,3,3)(up5)\n",
        "    # =========================================================================\n",
        "    up6 = UpSampling2D(size=(2, 2))(block5)\n",
        "    block6 = _conv_bn_relu_x2(64,3,3)(up6)\n",
        "    # =========================================================================\n",
        "    up7 = UpSampling2D(size=(2, 2))(block6)\n",
        "    block7 = _conv_bn_relu_x2(32,3,3)(up7)\n",
        "    return block7\n",
        "\n",
        "def fcrn_nalu(input):\n",
        "\n",
        "    # input-(256,256,3) or input_, output-(128,128,32)\n",
        "    block1 = _conv_bn_relu_x2(32,3,3)(input)\n",
        "    pool1 = MaxPooling2D(pool_size=(2,2))(block1)   \n",
        "    nal1 = NALU(32, mode=\"NAC\", \n",
        "             MW_initializer=RandomNormal(stddev=1),\n",
        "             G_initializer=Constant(10))(block1) # volume- (256,256,32)\n",
        "    # =========================================================================\n",
        "    # input-(128,128,32), output-(64,64,64)\n",
        "    block2 = _conv_bn_relu_x2(64,3,3)(pool1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(block2)\n",
        "    nal2 = NALU(64, mode=\"NAC\", \n",
        "             MW_initializer=RandomNormal(stddev=1),\n",
        "             G_initializer=Constant(10))(block2) # volume- (128,128,64)\n",
        "    # ========================================================================= \n",
        "    # input-(64,64,64), output-(32,32,128)\n",
        "    block3 = _conv_bn_relu_x2(128,3,3)(pool2)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(block3)\n",
        "    nal3 = NALU(128, mode=\"NAC\", \n",
        "             MW_initializer=RandomNormal(stddev=1),\n",
        "             G_initializer=Constant(10))(block3) # volume- (64,64,128)\n",
        "    # ========================================================================= \n",
        "    # input-(32,32,512), output-(64,64,128)# input-(32,32,128), output-(32,32,512)\n",
        "    block4 = _conv_bn_relu(512,3,3)(pool3)\n",
        "    # =========================================================================\n",
        "    # input-(32,32,512), output-(64,64,512)\n",
        "    up5 =  UpSampling2D(size=(2, 2))(block4)\n",
        "    # inputs-(64,64,512), (64,64,128)  output-(64,64,640)\n",
        "    block5 = concatenate([_conv_bn_relu_x2(128,3,3)(up5), nal3])\n",
        "    # inputs-(64,64,640), ->(64,64,128)  output-(64,64,128)\n",
        "    block5 = _conv_bn_relu_x2(128,3,3)(up5)\n",
        "    # ========================================================================= \n",
        "    # input-(64,64,128), output-(128,128,128)\n",
        "    up6 = UpSampling2D(size=(2, 2))(block5)\n",
        "    # inputs-(128,128,128), ->(128,128,64)  output-(128,128,192)\n",
        "    block6 = concatenate([_conv_bn_relu_x2(64,3,3)(up6), nal2])\n",
        "    # input-(128,128,192), output-(128,128,64)\n",
        "    block6 = _conv_bn_relu_x2(64,3,3)(up6)\n",
        "    # =========================================================================\n",
        "    # Compressing the dimension analysis going forward with upcoming layers & for U-net also.\n",
        "    # input-(64,64,128), output-(128,128,128)\n",
        "    up7 = UpSampling2D(size=(2, 2))(block6)\n",
        "    block7 = concatenate([_conv_bn_relu_x2(32,3,3)(up7), nal1])\n",
        "    block7 = _conv_bn_relu_x2(32,3,3)(up7)\n",
        "\n",
        "    return block7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqM7BRia_9fA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. U-net base and U-net/NALU base model structure decleration.\n",
        "\n",
        "def U_net_base(input, nb_filter = 64):\n",
        "    block1 = _conv_bn_relu_x2(nb_filter,3,3)(input)\n",
        "    pool1 = MaxPooling2D(pool_size=(2,2))(block1)\n",
        "    # =========================================================================\n",
        "    block2 = _conv_bn_relu_x2(128,3,3)(pool1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(block2)\n",
        "    # =========================================================================\n",
        "    block3 = _conv_bn_relu_x2(256,3,3)(pool2)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(block3)\n",
        "    # =========================================================================\n",
        "    block4 = _conv_bn_relu_x2(256,3,3)(pool3)\n",
        "    up4 = merge([UpSampling2D(size=(2, 2))(block4), block3], mode='concat', concat_axis=-1)\n",
        "    # =========================================================================\n",
        "    block5 = _conv_bn_relu_x2(128,3,3)(up4)\n",
        "    up5 = merge([UpSampling2D(size=(2, 2))(block5), block2], mode='concat', concat_axis=-1)\n",
        "    # =========================================================================\n",
        "    block6 = _conv_bn_relu_x2(nb_filter,3,3)(up5)\n",
        "    up6 = merge([UpSampling2D(size=(2, 2))(block6), block1], mode='concat', concat_axis=-1)\n",
        "    # =========================================================================\n",
        "    block7 = _conv_bn_relu(32,3,3)(up6)\n",
        "    return block7\n",
        "\n",
        "def u_net_nalu(input, nb_filter = 64):\n",
        "    # input-(256,256,3), output-(128,128,64)\n",
        "    block1 = _conv_bn_relu_x2(64,3,3)(input)\n",
        "    pool1 = MaxPooling2D(pool_size=(2,2))(block1)\n",
        "    nal1 = NALU(64, mode=\"NAC\", \n",
        "             MW_initializer=RandomNormal(stddev=1),\n",
        "             G_initializer=Constant(10))(block1) # volume- (256,256,64)\n",
        "    # =========================================================================\n",
        "    # input-(128,128,64), output-(64,64,128)\n",
        "    block2 = _conv_bn_relu_x2(128,3,3)(pool1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(block2)\n",
        "    nal2 = NALU(128, mode=\"NAC\", \n",
        "             MW_initializer=RandomNormal(stddev=1),\n",
        "             G_initializer=Constant(10))(block2) # volume- (128,128,128)\n",
        "    # ========================================================================= \n",
        "    # input-(64,64,128), output-(32,32,256)\n",
        "    block3 = _conv_bn_relu_x2(256,3,3)(pool2)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(block3)\n",
        "    nal3 = NALU(256, mode=\"NAC\", \n",
        "             MW_initializer=RandomNormal(stddev=1),\n",
        "             G_initializer=Constant(10))(block3) # volume- (64,64,256)\n",
        "    # =========================================================================\n",
        "    # input-(32,32,256), output-(64,64,256)\n",
        "    block4 = _conv_bn_relu_x2(256,3,3)(pool3)\n",
        "    up4 = concatenate([UpSampling2D(size=(2, 2))(block4), block3], axis=-1)\n",
        "    up4 = concatenate([up4, nal3], axis=-1)\n",
        "    up4 = _conv_bn_relu_x2(256,3,3)(up4)\n",
        "    # =========================================================================\n",
        "    # input-(64,64,256), output-(128,128,128)\n",
        "    block5 = _conv_bn_relu_x2(128,3,3)(up4)\n",
        "    up5 = concatenate([UpSampling2D(size=(2, 2))(block5), block2], axis=-1)\n",
        "    up5 = concatenate([up5, nal2], axis=-1)\n",
        "    up5 = _conv_bn_relu_x2(128,3,3)(up5)\n",
        "    # =========================================================================\n",
        "    # input-(128,128,128), output-(256,256,64)\n",
        "    block6 = _conv_bn_relu_x2(64,3,3)(up5)\n",
        "    up6 = concatenate([UpSampling2D(size=(2, 2))(block6), block1], axis=-1) # input-128, output-256\n",
        "    up6 = concatenate([up6, nal1], axis=-1)\n",
        "    up6 = _conv_bn_relu_x2(64,3,3)(up6)\n",
        "    # =========================================================================\n",
        "    # input-(256,256,64), output-(256,256,32)\n",
        "    block7 = _conv_bn_relu(32,3,3)(up6)\n",
        "    return block7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhlQwx28bN76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def buildmodel_fcrn_nalu (input_dim):\n",
        "    input_ = Input (shape = (input_dim))\n",
        "    # =========================================================================\n",
        "    # Change the base here to train different model base structure.\n",
        "    act_ = fcrn_nalu (input_)\n",
        "    # =========================================================================\n",
        "    density_pred =  Convolution2D(1, 1, 1, bias = False, activation='linear',\\\n",
        "                                  init='orthogonal',name='pred',border_mode='same')(act_)\n",
        "    # =========================================================================\n",
        "    model = Model (input = input_, output = density_pred)\n",
        "    opt = SGD(lr = 1e-2, momentum = 0.9, nesterov = True)\n",
        "    model.compile(optimizer = opt, loss = 'mse')\n",
        "    return model\n",
        "\n",
        "def buildmodel_u_net_nalu (input_dim):\n",
        "    input_ = Input (shape = (input_dim))\n",
        "    # =========================================================================\n",
        "    # Change the base here to train different model base structure.\n",
        "    act_ = u_net_nalu (input_, nb_filter = 64 )\n",
        "    # =========================================================================\n",
        "    density_pred =  Convolution2D(1, 1, 1, bias = False, activation='linear',\\\n",
        "                                  init='orthogonal',name='pred',border_mode='same')(act_)\n",
        "    # =========================================================================\n",
        "    model = Model (input = input_, output = density_pred)\n",
        "    opt = SGD(lr = 1e-2, momentum = 0.9, nesterov = True)\n",
        "    model.compile(optimizer = opt, loss = 'mse')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYMlflzDyTJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data augmentation functions associated \n",
        "from generator import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TWil_jZbN3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pdb\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint,Callback,LearningRateScheduler\n",
        "from scipy import misc\n",
        "import imageio\n",
        "import scipy.ndimage as ndimage\n",
        "\n",
        "class LossHistory(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "\n",
        "base_path = 'bbbc005-cells/'\n",
        "data = []\n",
        "anno = []\n",
        "\n",
        "def step_decay(epoch):\n",
        "    # step decay implementation.\n",
        "    step = 16\n",
        "    num =  epoch // step \n",
        "    if num % 3 == 0:\n",
        "        lrate = 1e-3\n",
        "    elif num % 3 == 1:\n",
        "        lrate = 1e-4\n",
        "    else:\n",
        "        lrate = 1e-5\n",
        "\n",
        "        # continous decaying implementation. decay lrate first time and then assign new learning with every \n",
        "        # initial_lrate = lrate\n",
        "        # lrate = initial_lrate * 1/(1 + decay * (epoch - num * step))\n",
        "    print('Learning rate for epoch {} is {}.'.format(epoch+1, lrate))    \n",
        "    return np.float(lrate)\n",
        "\n",
        "def read_data(base_path):\n",
        "    imList = os.listdir(base_path)\n",
        "    for i in range(len(imList)): \n",
        "        if 'cell' in imList[i]:\n",
        "            img1 = imageio.imread(os.path.join(base_path,imList[i]))\n",
        "            data.append(img1)\n",
        "            \n",
        "            img2_ = imageio.imread(os.path.join(base_path, imList[i][:3] + 'dots.jpg'))\n",
        "            # amplify the dot annotation values in R channel of RGB.\n",
        "            # Omit this amplification step if you are trying to train this model on other images.\n",
        "            # BBBC005 dataset doesn't require amplitude increment for any pixel value like flourescent synthetic dataset.\n",
        "            img2_ = 1.0 * (img2_[:,:,0] > 0)\n",
        "            img2 = ndimage.gaussian_filter(img2_, sigma=(1, 1), order=0)\n",
        "            anno.append(img2)\n",
        "    return np.asarray(data, dtype = 'float32'), np.asarray(anno, dtype = 'float32')\n",
        "\n",
        "\n",
        "def train_(base_path):\n",
        "    data, anno = read_data(base_path)\n",
        "    anno = np.expand_dims(anno, axis = -1)\n",
        "    \n",
        "    mean = np.mean(data)\n",
        "    std = np.std(data)\n",
        "    \n",
        "    data_ = (data - mean) / std\n",
        "    \n",
        "    train_data = data_[:225]\n",
        "    train_anno = anno[:225]\n",
        "\n",
        "    val_data = data_[225:]\n",
        "    val_anno = anno[225:]\n",
        "    \n",
        "    print('-'*30)\n",
        "    print('Creating and compiling the fully convolutional regression networks.')\n",
        "    print('-'*30)    \n",
        "   \n",
        "    # model building step: change this line to build new model each time.\n",
        "    model = buildmodel_u_net_nalu(input_dim = (256,256,3))\n",
        "    # model = buildmodel_fcrn_nalu(input_dim = (256,256,3))\n",
        "\n",
        "    # Also, change this line for saving a new variant of a model while training.\n",
        "    # Here, a demo model is just executed for getting started.\n",
        "    model_checkpoint = ModelCheckpoint('cell_counting_demo_run.hdf5', monitor='loss', save_best_only=True)\n",
        "    model.summary()\n",
        "    print('...Fitting model...')\n",
        "    print('-'*30)\n",
        "    change_lr = LearningRateScheduler(step_decay)\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center = False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center = False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization = False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization = False,  # divide each input by its std\n",
        "        zca_whitening = False,  # apply ZCA whitening\n",
        "        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range = 0.3,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range = 0.3,  # randomly shift images vertically (fraction of total height)\n",
        "        zoom_range = 0.3,\n",
        "        shear_range = 0.,\n",
        "        horizontal_flip = True,  # randomly flip images\n",
        "        vertical_flip = True, # randomly flip images\n",
        "        fill_mode = 'constant',\n",
        "        dim_ordering = 'tf')  \n",
        "\n",
        "    # list of epochs batches to measure MAE after training & stopping the training after appropriate epochs and avoiding overfitting.\n",
        "    # Assumption for this MAE measurement approach is that model definitely achieves ideal prediction after 144 epochs.\n",
        "    # An epoch offset list logic to determine models performance at best and stopping that point.\n",
        "    lst_epochs = [144,16,16,16,16,16,16,16,8,8,8,8,8]\n",
        "    # Global error value to capture the minimum most error value on test set.\n",
        "    global_err_val = 100.0\n",
        "    # This measures offset value of epoch in which the training has to be stopped.\n",
        "    ep_offset = 0\n",
        "    for epoch in lst_epochs:\n",
        "        ep_offset = epoch # assuming epochs required for training is greater than 144 always. A reasonable assumption for this dataset.\n",
        "        model.fit_generator(datagen.flow(train_data,\n",
        "                                        train_anno,\n",
        "                                        batch_size = 16\n",
        "                                        ),\n",
        "                            samples_per_epoch = train_data.shape[0],\n",
        "                            nb_epoch = epoch,\n",
        "                            callbacks = [model_checkpoint, change_lr],\n",
        "                        )\n",
        "    \n",
        "        model.load_weights('cell_counting_demo_run.hdf5')\n",
        "        A = model.predict(val_data)    \n",
        "        mean_diff = np.average(  np.abs(  np.sum(np.sum(A,1),1)-np.sum(np.sum(val_anno,1),1)  )  ) / (300.0)\n",
        "        if mean_diff < global_err_val:\n",
        "            global_err_val = mean_diff\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    print('After training, the mean absolute error is : {} cells per image.'.format(np.abs(global_err_val)))\n",
        "    print('This error is obtained after training for: '+str(144+int(ep_offset)) + ' epochs' )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTp_7DtrvgF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    train_(base_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H82AIrS5vf_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}