{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cell-counting-model-training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4et4eWn9UTFM",
        "colab_type": "code",
        "outputId": "b313bf9d-5fe7-4654-8e69-a0c85dffaea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "341gq47HUc4U",
        "colab_type": "code",
        "outputId": "b8339d66-12d8-441f-ab92-60163636420e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/gdrive/My\\ Drive/bbbc-005-dataset"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/bbbc-005-dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iAr4iNkBjvp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b8db34aa-d985-40be-e604-530c527b1345"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BBBC005_cells_dataset_final.tar.xz  cell-counting-model-training.ipynb\n",
            "BBBC005_cells_dataset_final.zip     generator.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4zNb9DlUc7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xvf  BBBC005_cells_dataset_final.tar.xz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD5_NjN0BpeT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "82fd45a3-dc16-45cf-f1de-419374c6d9e6"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BBBC005_cells_dataset_final\t    cell-counting-model-training.ipynb\n",
            "BBBC005_cells_dataset_final.tar.xz  generator.py\n",
            "BBBC005_cells_dataset_final.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDBhNv8wUc_2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43942313-a072-49c7-86b2-9929228a84d7"
      },
      "source": [
        "# import statements for constructing the model design.\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import (\n",
        "    Input,\n",
        "    Activation,\n",
        "    concatenate,\n",
        "    add,\n",
        "    merge,\n",
        "    Dropout,\n",
        "    Reshape,\n",
        "    Permute,\n",
        "    Dense,\n",
        "    UpSampling2D,\n",
        "    Flatten\n",
        "    )\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers.convolutional import (\n",
        "    Convolution2D)\n",
        "from keras.layers.pooling import (\n",
        "    MaxPooling2D,\n",
        "    AveragePooling2D\n",
        "    )\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras.layers import *\n",
        "from keras.initializers import *\n",
        "from keras.models import *\n",
        "from keras import initializers"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSoeHFu_yxxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code is referenced from: [kgrm](https://github.com/kgrm)\n",
        "class NALU(Layer):\n",
        "    def __init__(self, units, MW_initializer='glorot_uniform',\n",
        "                 G_initializer='glorot_uniform', mode=\"NALU\",\n",
        "                 **kwargs):\n",
        "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
        "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
        "        super(NALU, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.mode = mode\n",
        "        self.MW_initializer = initializers.get(MW_initializer)\n",
        "        self.G_initializer = initializers.get(G_initializer)\n",
        "        self.input_spec = InputSpec(min_ndim=2)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 2\n",
        "        input_dim = input_shape[-1]\n",
        "\n",
        "        self.W_hat = self.add_weight(shape=(input_dim, self.units),\n",
        "                                     initializer=self.MW_initializer,\n",
        "                                     name='W_hat')\n",
        "        self.M_hat = self.add_weight(shape=(input_dim, self.units),\n",
        "                                     initializer=self.MW_initializer,\n",
        "                                     name='M_hat')\n",
        "        if self.mode == \"NALU\":\n",
        "            self.G = self.add_weight(shape=(input_dim, self.units),\n",
        "                                     initializer=self.G_initializer,\n",
        "                                     name='G')\n",
        "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        W = K.tanh(self.W_hat) * K.sigmoid(self.M_hat)\n",
        "        a = K.dot(inputs, W)\n",
        "        if self.mode == \"NAC\":\n",
        "            output = a\n",
        "        elif self.mode == \"NALU\":\n",
        "            m = K.exp(K.dot(K.log(K.abs(inputs) + 1e-7), W))\n",
        "            g = K.sigmoid(K.dot(K.abs(inputs), self.G))\n",
        "            output = g * a + (1 - g) * m\n",
        "        else:\n",
        "            raise ValueError(\"Valid modes: 'NAC', 'NALU'.\")\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert input_shape and len(input_shape) >= 2\n",
        "        assert input_shape[-1]\n",
        "        output_shape = list(input_shape)\n",
        "        output_shape[-1] = self.units\n",
        "        return tuple(output_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq0fZemdyyNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# weight decay defined on model's parametric matrices for regularization purpose.\n",
        "weight_decay = 1e-5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6yMq_5f6nHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _conv_bn_relu(nb_filter, row, col, subsample = (1,1)):\n",
        "    def f(input):\n",
        "        conv_a = Convolution2D(nb_filter, row, col, subsample = subsample,\n",
        "                               init = 'orthogonal', \n",
        "                               border_mode='same', bias = False)(input)\n",
        "        norm_a = BatchNormalization()(conv_a)\n",
        "        # 1. This actionvation function can be changed as demonstrated in experiments to create FCRN/U-net variants.\n",
        "        act_a = Activation(activation = 'relu')(norm_a)\n",
        "        return act_a\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVmrcYtwyyVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _conv_bn_relu_x2(nb_filter, row, col, subsample = (1,1)):\n",
        "    def f(input):\n",
        "        # 1. batch normalization with relu activation, again it can be changed to create different variants of FCRN & U-net.\n",
        "        conv_a = Convolution2D(nb_filter, row, col, subsample = subsample,\n",
        "                               init = 'orthogonal', border_mode = 'same',bias = False,\n",
        "                               W_regularizer = l2(weight_decay),\n",
        "                               b_regularizer = l2(weight_decay))(input)\n",
        "        norm_a = BatchNormalization()(conv_a)\n",
        "        act_a = Activation(activation = 'relu')(norm_a)\n",
        "        conv_b = Convolution2D(nb_filter, row, col, subsample = subsample,\n",
        "                              init = 'orthogonal', border_mode = 'same',bias = False,\n",
        "                              W_regularizer = l2(weight_decay),\n",
        "                              b_regularizer = l2(weight_decay))(act_a)\n",
        "        norm_b = BatchNormalization()(conv_b)\n",
        "        act_b = Activation(activation = 'relu')(norm_b)\n",
        "        return act_a\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-nRWlnsyyT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def U_net_base(input, nb_filter = 64):\n",
        "    block1 = _conv_bn_relu_x2(nb_filter,3,3)(input)\n",
        "    pool1 = MaxPooling2D(pool_size=(2,2))(block1)\n",
        "    # =========================================================================\n",
        "    block2 = _conv_bn_relu_x2(128,3,3)(pool1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(block2)\n",
        "    # =========================================================================\n",
        "    block3 = _conv_bn_relu_x2(256,3,3)(pool2)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(block3)\n",
        "    # =========================================================================\n",
        "    block4 = _conv_bn_relu_x2(256,3,3)(pool3)\n",
        "    up4 = merge([UpSampling2D(size=(2, 2))(block4), block3], mode='concat', concat_axis=-1)\n",
        "    # =========================================================================\n",
        "    block5 = _conv_bn_relu_x2(128,3,3)(up4)\n",
        "    up5 = merge([UpSampling2D(size=(2, 2))(block5), block2], mode='concat', concat_axis=-1)\n",
        "    # =========================================================================\n",
        "    block6 = _conv_bn_relu_x2(nb_filter,3,3)(up5)\n",
        "    up6 = merge([UpSampling2D(size=(2, 2))(block6), block1], mode='concat', concat_axis=-1)\n",
        "    # =========================================================================\n",
        "    block7 = _conv_bn_relu(32,3,3)(up6)\n",
        "    return block7\n",
        "\n",
        "def u_net_nalu(input, nb_filter = 64):\n",
        "    # input-(256,256,3), output-(128,128,64)\n",
        "    block1 = _conv_bn_relu_x2(64,3,3)(input)\n",
        "    pool1 = MaxPooling2D(pool_size=(2,2))(block1)\n",
        "    nal1 = NALU(64, mode=\"NAC\", \n",
        "             MW_initializer=RandomNormal(stddev=1),\n",
        "             G_initializer=Constant(10))(block1) # volume- (256,256,64)\n",
        "    # =========================================================================\n",
        "    # input-(128,128,64), output-(64,64,128)\n",
        "    block2 = _conv_bn_relu_x2(128,3,3)(pool1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(block2)\n",
        "    nal2 = NALU(128, mode=\"NAC\", \n",
        "             MW_initializer=RandomNormal(stddev=1),\n",
        "             G_initializer=Constant(10))(block2) # volume- (128,128,128)\n",
        "    # ========================================================================= \n",
        "    # input-(64,64,128), output-(32,32,256)\n",
        "    block3 = _conv_bn_relu_x2(256,3,3)(pool2)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(block3)\n",
        "    nal3 = NALU(256, mode=\"NAC\", \n",
        "             MW_initializer=RandomNormal(stddev=1),\n",
        "             G_initializer=Constant(10))(block3) # volume- (64,64,256)\n",
        "    # =========================================================================\n",
        "    # input-(32,32,256), output-(64,64,256)\n",
        "    block4 = _conv_bn_relu_x2(256,3,3)(pool3)\n",
        "    up4 = concatenate([UpSampling2D(size=(2, 2))(block4), block3], axis=-1)\n",
        "    up4 = concatenate([up4, nal3], axis=-1)\n",
        "    up4 = _conv_bn_relu_x2(256,3,3)(up4)\n",
        "    # =========================================================================\n",
        "    # input-(64,64,256), output-(128,128,128)\n",
        "    block5 = _conv_bn_relu_x2(128,3,3)(up4)\n",
        "    up5 = concatenate([UpSampling2D(size=(2, 2))(block5), block2], axis=-1)\n",
        "    up5 = concatenate([up5, nal2], axis=-1)\n",
        "    up5 = _conv_bn_relu_x2(128,3,3)(up5)\n",
        "    # =========================================================================\n",
        "    # input-(128,128,128), output-(256,256,64)\n",
        "    block6 = _conv_bn_relu_x2(64,3,3)(up5)\n",
        "    up6 = concatenate([UpSampling2D(size=(2, 2))(block6), block1], axis=-1) # input-128, output-256\n",
        "    up6 = concatenate([up6, nal1], axis=-1)\n",
        "    up6 = _conv_bn_relu_x2(64,3,3)(up6)\n",
        "    # =========================================================================\n",
        "    # input-(256,256,64), output-(256,256,32)\n",
        "    block7 = _conv_bn_relu(32,3,3)(up6)\n",
        "    return block7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZINWTkO4wN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def buildmodel_u_net (input_dim):\n",
        "    input_ = Input (shape = (input_dim))\n",
        "    # =========================================================================\n",
        "    # Change the base here to train different model base structure.\n",
        "    act_ = U_net_base (input_, nb_filter = 64 )\n",
        "    # =========================================================================\n",
        "    density_pred =  Convolution2D(1, 1, 1, bias = False, activation='linear',\\\n",
        "                                  init='orthogonal',name='pred',border_mode='same')(act_)\n",
        "    # =========================================================================\n",
        "    model = Model (input = input_, output = density_pred)\n",
        "    opt = SGD(lr = 1e-2, momentum = 0.9, nesterov = True)\n",
        "    model.compile(optimizer = opt, loss = 'mse')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3Tk5rR_yyQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def buildmodel_u_net_nalu (input_dim):\n",
        "    input_ = Input (shape = (input_dim))\n",
        "    # =========================================================================\n",
        "    # Change the base here to train different model base structure.\n",
        "    act_ = u_net_nalu (input_, nb_filter = 64 )\n",
        "    # =========================================================================\n",
        "    density_pred =  Convolution2D(1, 1, 1, bias = False, activation='linear',\\\n",
        "                                  init='orthogonal',name='pred',border_mode='same')(act_)\n",
        "    # =========================================================================\n",
        "    model = Model (input = input_, output = density_pred)\n",
        "    opt = SGD(lr = 1e-2, momentum = 0.9, nesterov = True)\n",
        "    model.compile(optimizer = opt, loss = 'mse')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "544PZBjwyyLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from generator import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeHXfekCyyJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pdb\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint,Callback,LearningRateScheduler\n",
        "from scipy import misc\n",
        "import imageio\n",
        "import scipy.ndimage as ndimage\n",
        "\n",
        "class LossHistory(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "\n",
        "base_path = 'BBBC005_cells_dataset_final/'\n",
        "data = []\n",
        "anno = []\n",
        "\n",
        "def step_decay(epoch):\n",
        "    # step decay implementation.\n",
        "    step = 16\n",
        "    num =  epoch // step \n",
        "    if num % 3 == 0:\n",
        "        lrate = 1e-3\n",
        "    elif num % 3 == 1:\n",
        "        lrate = 1e-4\n",
        "    else:\n",
        "        lrate = 1e-5\n",
        "\n",
        "        # continous decaying implementation. decay lrate first time and then assign new learning with every \n",
        "        # initial_lrate = lrate\n",
        "        # lrate = initial_lrate * 1/(1 + decay * (epoch - num * step))\n",
        "    print('Learning rate for epoch {} is {}.'.format(epoch+1, lrate))    \n",
        "    return np.float(lrate)\n",
        "\n",
        "def read_data(base_path):\n",
        "    imList = os.listdir(base_path)\n",
        "    for i in range(len(imList)): \n",
        "        if 'cell' in imList[i]:\n",
        "            img1 = imageio.imread(os.path.join(base_path,imList[i]))\n",
        "            data.append(img1)\n",
        "            \n",
        "            img2_ = imageio.imread(os.path.join(base_path, imList[i][:4] + 'dots.png'))\n",
        "            # amplify the dot annotation values in R channel of RGB.\n",
        "            # Omit this amplification step if you are trying to train this model on other images.\n",
        "            # BBBC005 dataset doesn't require amplitude increment for any pixel value like flourescent synthetic dataset.\n",
        "            img2_ = 1.0 * (img2_[:,:,0] > 0)\n",
        "            img2 = ndimage.gaussian_filter(img2_, sigma=(1, 1), order=0)\n",
        "            anno.append(img2)\n",
        "    return np.asarray(data, dtype = 'float32'), np.asarray(anno, dtype = 'float32')\n",
        "\n",
        "\n",
        "def train_(base_path):\n",
        "    data, anno = read_data(base_path)\n",
        "    anno = np.expand_dims(anno, axis = -1)\n",
        "    \n",
        "    mean = np.mean(data)\n",
        "    std = np.std(data)\n",
        "    \n",
        "    data_ = (data - mean) / std\n",
        "    \n",
        "    train_data = data_[:1470]\n",
        "    train_anno = anno[:1470]\n",
        "\n",
        "    val_data = data_[1470:]\n",
        "    val_anno = anno[1470:]\n",
        "    \n",
        "    print('-'*30)\n",
        "    print('Creating and compiling the fully convolutional regression networks.')\n",
        "    print('-'*30)    \n",
        "   \n",
        "    # model building step: change this line to build new model each time.\n",
        "    model = buildmodel_u_net_nalu(input_dim = (256,256,3))\n",
        "    # model = buildmodel_u_net(input_dim = (256,256,3))\n",
        "\n",
        "    # Also, change this line for saving a new variant of a model while training.\n",
        "    # Here, a demo model is just executed for getting started.\n",
        "    model_checkpoint = ModelCheckpoint('cell_counting_unet_nalu.hdf5', monitor='loss', save_best_only=True)\n",
        "    model.summary()\n",
        "    print('...Fitting model...')\n",
        "    print('-'*30)\n",
        "    change_lr = LearningRateScheduler(step_decay)\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center = False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center = False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization = False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization = False,  # divide each input by its std\n",
        "        zca_whitening = False,  # apply ZCA whitening\n",
        "        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range = 0.3,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range = 0.3,  # randomly shift images vertically (fraction of total height)\n",
        "        zoom_range = 0.3,\n",
        "        shear_range = 0.,\n",
        "        horizontal_flip = True,  # randomly flip images\n",
        "        vertical_flip = True, # randomly flip images\n",
        "        fill_mode = 'constant',\n",
        "        dim_ordering = 'tf')  \n",
        "\n",
        "    # list of epochs batches to measure MAE after training & stopping the training after appropriate epochs and avoiding overfitting.\n",
        "    # Assumption for this MAE measurement approach is that model definitely achieves ideal prediction after 64 epochs.\n",
        "    # An epoch offset list logic to determine models performance at best and stopping that point.\n",
        "    lst_epochs = [64,16,8,16,8,16,8,16,8,16,8,16,8,16,8,8,8,8,8,8,4,4,2,2]\n",
        "    # Global error value to capture the minimum most error value on test set.\n",
        "    global_err_val = 100.0\n",
        "    # This measures offset value of epoch in which the training has to be stopped.\n",
        "    ep_offset = 0\n",
        "    for epoch in lst_epochs:\n",
        "        ep_offset = epoch # assuming epochs required for training is greater than 64 always. A reasonable assumption for this dataset.\n",
        "        model.fit_generator(datagen.flow(train_data,\n",
        "                                        train_anno,\n",
        "                                        batch_size = 16\n",
        "                                        ),\n",
        "                            samples_per_epoch = train_data.shape[0],\n",
        "                            nb_epoch = epoch,\n",
        "                            callbacks = [model_checkpoint, change_lr],\n",
        "                        )\n",
        "    \n",
        "        model.load_weights('cell_counting_unet_nalu.hdf5')\n",
        "        A = model.predict(val_data)    \n",
        "        mean_diff = np.average(  np.abs(  np.sum(np.sum(A,1),1)-np.sum(np.sum(val_anno,1),1)  )  ) / (630.0)\n",
        "        if mean_diff < global_err_val:\n",
        "            global_err_val = mean_diff\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    print('After training, the mean absolute error is : {} cells per image.'.format(np.abs(global_err_val)))\n",
        "    print('This error is obtained after training for: '+str(64+int(ep_offset)) + ' epochs' )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTtDPPPByyHV",
        "colab_type": "code",
        "outputId": "a77b3d4b-0b9a-4e1b-8e0f-eb652fa6d9b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    train_(base_path)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Creating and compiling the fully convolutional regression networks.\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\", kernel_initializer=\"orthogonal\", kernel_regularizer=<keras.reg..., bias_regularizer=<keras.reg..., use_bias=False)`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\", kernel_initializer=\"orthogonal\", kernel_regularizer=<keras.reg..., bias_regularizer=<keras.reg..., use_bias=False)`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), strides=(1, 1), padding=\"same\", kernel_initializer=\"orthogonal\", kernel_regularizer=<keras.reg..., bias_regularizer=<keras.reg..., use_bias=False)`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), strides=(1, 1), padding=\"same\", kernel_initializer=\"orthogonal\", kernel_regularizer=<keras.reg..., bias_regularizer=<keras.reg..., use_bias=False)`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), strides=(1, 1), padding=\"same\", kernel_initializer=\"orthogonal\", kernel_regularizer=<keras.reg..., bias_regularizer=<keras.reg..., use_bias=False)`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), strides=(1, 1), padding=\"same\", kernel_initializer=\"orthogonal\", kernel_regularizer=<keras.reg..., bias_regularizer=<keras.reg..., use_bias=False)`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), strides=(1, 1), padding=\"same\", kernel_initializer=\"orthogonal\", use_bias=False)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (1, 1), activation=\"linear\", name=\"pred\", padding=\"same\", kernel_initializer=\"orthogonal\", use_bias=False)`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"pr...)`\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:119: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:119: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., callbacks=[<keras.ca..., steps_per_epoch=91, epochs=64)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 256, 256, 64) 1728        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 256, 256, 64) 256         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 256, 256, 64) 0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 128, 128, 64) 0           activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 128, 128, 128 73728       max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 128, 128, 128 512         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 128, 128, 128 0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 64, 64, 128)  0           activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 64, 64, 256)  294912      max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 64, 64, 256)  1024        conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 64, 64, 256)  0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 32, 32, 256)  0           activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 32, 32, 256)  589824      max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 32, 32, 256)  1024        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 32, 32, 256)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2D)  (None, 64, 64, 256)  0           activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 64, 64, 512)  0           up_sampling2d_4[0][0]            \n",
            "                                                                 activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "nalu_6 (NALU)                   (None, 64, 64, 256)  131072      activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 64, 64, 768)  0           concatenate_7[0][0]              \n",
            "                                                                 nalu_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 64, 64, 256)  1769472     concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 64, 64, 256)  1024        conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 64, 64, 256)  0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 64, 64, 128)  294912      activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 64, 64, 128)  512         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 64, 64, 128)  0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_5 (UpSampling2D)  (None, 128, 128, 128 0           activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 128, 128, 256 0           up_sampling2d_5[0][0]            \n",
            "                                                                 activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "nalu_5 (NALU)                   (None, 128, 128, 128 32768       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 128, 128, 384 0           concatenate_9[0][0]              \n",
            "                                                                 nalu_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 128, 128, 128 442368      concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 128, 128, 128 512         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 128, 128, 128 0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 128, 128, 64) 73728       activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 128, 128, 64) 256         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 128, 128, 64) 0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_6 (UpSampling2D)  (None, 256, 256, 64) 0           activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 256, 256, 128 0           up_sampling2d_6[0][0]            \n",
            "                                                                 activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "nalu_4 (NALU)                   (None, 256, 256, 64) 8192        activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 256, 256, 192 0           concatenate_11[0][0]             \n",
            "                                                                 nalu_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 256, 256, 64) 110592      concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 256, 256, 64) 256         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 256, 256, 64) 0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 256, 256, 32) 18432       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 256, 256, 32) 128         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 256, 256, 32) 0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "pred (Conv2D)                   (None, 256, 256, 1)  32          activation_38[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 3,847,264\n",
            "Trainable params: 3,844,512\n",
            "Non-trainable params: 2,752\n",
            "__________________________________________________________________________________________________\n",
            "...Fitting model...\n",
            "------------------------------\n",
            "Epoch 1/64\n",
            "Learning rate for epoch 1 is 0.001.\n",
            "91/91 [==============================] - 40s 439ms/step - loss: 0.1173\n",
            "Epoch 2/64\n",
            "Learning rate for epoch 2 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0421\n",
            "Epoch 3/64\n",
            "Learning rate for epoch 3 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0374\n",
            "Epoch 4/64\n",
            "Learning rate for epoch 4 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0350\n",
            "Epoch 5/64\n",
            "Learning rate for epoch 5 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0334\n",
            "Epoch 6/64\n",
            "Learning rate for epoch 6 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0328\n",
            "Epoch 7/64\n",
            "Learning rate for epoch 7 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0322\n",
            "Epoch 8/64\n",
            "Learning rate for epoch 8 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0314\n",
            "Epoch 9/64\n",
            "Learning rate for epoch 9 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0315\n",
            "Epoch 10/64\n",
            "Learning rate for epoch 10 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0304\n",
            "Epoch 11/64\n",
            "Learning rate for epoch 11 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0305\n",
            "Epoch 12/64\n",
            "Learning rate for epoch 12 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0304\n",
            "Epoch 13/64\n",
            "Learning rate for epoch 13 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0293\n",
            "Epoch 14/64\n",
            "Learning rate for epoch 14 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0302\n",
            "Epoch 15/64\n",
            "Learning rate for epoch 15 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0298\n",
            "Epoch 16/64\n",
            "Learning rate for epoch 16 is 0.001.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0291\n",
            "Epoch 17/64\n",
            "Learning rate for epoch 17 is 0.0001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0291\n",
            "Epoch 18/64\n",
            "Learning rate for epoch 18 is 0.0001.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0293\n",
            "Epoch 19/64\n",
            "Learning rate for epoch 19 is 0.0001.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0293\n",
            "Epoch 20/64\n",
            "Learning rate for epoch 20 is 0.0001.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0294\n",
            "Epoch 21/64\n",
            "Learning rate for epoch 21 is 0.0001.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0294\n",
            "Epoch 22/64\n",
            "Learning rate for epoch 22 is 0.0001.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0292\n",
            "Epoch 23/64\n",
            "Learning rate for epoch 23 is 0.0001.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0292\n",
            "Epoch 24/64\n",
            "Learning rate for epoch 24 is 0.0001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0281\n",
            "Epoch 25/64\n",
            "Learning rate for epoch 25 is 0.0001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0295\n",
            "Epoch 26/64\n",
            "Learning rate for epoch 26 is 0.0001.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0290\n",
            "Epoch 27/64\n",
            "Learning rate for epoch 27 is 0.0001.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0294\n",
            "Epoch 28/64\n",
            "Learning rate for epoch 28 is 0.0001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0295\n",
            "Epoch 29/64\n",
            "Learning rate for epoch 29 is 0.0001.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0298\n",
            "Epoch 30/64\n",
            "Learning rate for epoch 30 is 0.0001.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0287\n",
            "Epoch 31/64\n",
            "Learning rate for epoch 31 is 0.0001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0287\n",
            "Epoch 32/64\n",
            "Learning rate for epoch 32 is 0.0001.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0290\n",
            "Epoch 33/64\n",
            "Learning rate for epoch 33 is 1e-05.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0290\n",
            "Epoch 34/64\n",
            "Learning rate for epoch 34 is 1e-05.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0288\n",
            "Epoch 35/64\n",
            "Learning rate for epoch 35 is 1e-05.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0293\n",
            "Epoch 36/64\n",
            "Learning rate for epoch 36 is 1e-05.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0288\n",
            "Epoch 37/64\n",
            "Learning rate for epoch 37 is 1e-05.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0288\n",
            "Epoch 38/64\n",
            "Learning rate for epoch 38 is 1e-05.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0287\n",
            "Epoch 39/64\n",
            "Learning rate for epoch 39 is 1e-05.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0288\n",
            "Epoch 40/64\n",
            "Learning rate for epoch 40 is 1e-05.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0296\n",
            "Epoch 41/64\n",
            "Learning rate for epoch 41 is 1e-05.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0289\n",
            "Epoch 42/64\n",
            "Learning rate for epoch 42 is 1e-05.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0293\n",
            "Epoch 43/64\n",
            "Learning rate for epoch 43 is 1e-05.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0287\n",
            "Epoch 44/64\n",
            "Learning rate for epoch 44 is 1e-05.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0287\n",
            "Epoch 45/64\n",
            "Learning rate for epoch 45 is 1e-05.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0296\n",
            "Epoch 46/64\n",
            "Learning rate for epoch 46 is 1e-05.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0284\n",
            "Epoch 47/64\n",
            "Learning rate for epoch 47 is 1e-05.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0287\n",
            "Epoch 48/64\n",
            "Learning rate for epoch 48 is 1e-05.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0294\n",
            "Epoch 49/64\n",
            "Learning rate for epoch 49 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0288\n",
            "Epoch 50/64\n",
            "Learning rate for epoch 50 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0287\n",
            "Epoch 51/64\n",
            "Learning rate for epoch 51 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0287\n",
            "Epoch 52/64\n",
            "Learning rate for epoch 52 is 0.001.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0298\n",
            "Epoch 53/64\n",
            "Learning rate for epoch 53 is 0.001.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0284\n",
            "Epoch 54/64\n",
            "Learning rate for epoch 54 is 0.001.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0284\n",
            "Epoch 55/64\n",
            "Learning rate for epoch 55 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0289\n",
            "Epoch 56/64\n",
            "Learning rate for epoch 56 is 0.001.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0280\n",
            "Epoch 57/64\n",
            "Learning rate for epoch 57 is 0.001.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0281\n",
            "Epoch 58/64\n",
            "Learning rate for epoch 58 is 0.001.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0284\n",
            "Epoch 59/64\n",
            "Learning rate for epoch 59 is 0.001.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0284\n",
            "Epoch 60/64\n",
            "Learning rate for epoch 60 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0276\n",
            "Epoch 61/64\n",
            "Learning rate for epoch 61 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0279\n",
            "Epoch 62/64\n",
            "Learning rate for epoch 62 is 0.001.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0282\n",
            "Epoch 63/64\n",
            "Learning rate for epoch 63 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0276\n",
            "Epoch 64/64\n",
            "Learning rate for epoch 64 is 0.001.\n",
            "91/91 [==============================] - 38s 420ms/step - loss: 0.0275\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:119: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., callbacks=[<keras.ca..., steps_per_epoch=91, epochs=16)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/16\n",
            "Learning rate for epoch 1 is 0.001.\n",
            "91/91 [==============================] - 38s 423ms/step - loss: 0.0279\n",
            "Epoch 2/16\n",
            "Learning rate for epoch 2 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0278\n",
            "Epoch 3/16\n",
            "Learning rate for epoch 3 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0275\n",
            "Epoch 4/16\n",
            "Learning rate for epoch 4 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0274\n",
            "Epoch 5/16\n",
            "Learning rate for epoch 5 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0282\n",
            "Epoch 6/16\n",
            "Learning rate for epoch 6 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0275\n",
            "Epoch 7/16\n",
            "Learning rate for epoch 7 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0268\n",
            "Epoch 8/16\n",
            "Learning rate for epoch 8 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0277\n",
            "Epoch 9/16\n",
            "Learning rate for epoch 9 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0277\n",
            "Epoch 10/16\n",
            "Learning rate for epoch 10 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0270\n",
            "Epoch 11/16\n",
            "Learning rate for epoch 11 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0273\n",
            "Epoch 12/16\n",
            "Learning rate for epoch 12 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0277\n",
            "Epoch 13/16\n",
            "Learning rate for epoch 13 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0270\n",
            "Epoch 14/16\n",
            "Learning rate for epoch 14 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0273\n",
            "Epoch 15/16\n",
            "Learning rate for epoch 15 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0269\n",
            "Epoch 16/16\n",
            "Learning rate for epoch 16 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0272\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:119: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., callbacks=[<keras.ca..., steps_per_epoch=91, epochs=8)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "Learning rate for epoch 1 is 0.001.\n",
            "91/91 [==============================] - 39s 424ms/step - loss: 0.0273\n",
            "Epoch 2/8\n",
            "Learning rate for epoch 2 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0275\n",
            "Epoch 3/8\n",
            "Learning rate for epoch 3 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0271\n",
            "Epoch 4/8\n",
            "Learning rate for epoch 4 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0275\n",
            "Epoch 5/8\n",
            "Learning rate for epoch 5 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0273\n",
            "Epoch 6/8\n",
            "Learning rate for epoch 6 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0272\n",
            "Epoch 7/8\n",
            "Learning rate for epoch 7 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0271\n",
            "Epoch 8/8\n",
            "Learning rate for epoch 8 is 0.001.\n",
            "91/91 [==============================] - 38s 421ms/step - loss: 0.0274\n",
            "After training, the mean absolute error is : 3.9764989459325397 cells per image.\n",
            "This error is obtained after training for: 72 epochs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjANcX5_yx3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Final Result for U-net NALU based model is stated for this dataset.\n",
        "# ...\n",
        "# Learning rate for epoch 8 is 0.001.\n",
        "# 91/91 [==============================] - 38s 421ms/step - loss: 0.0274\n",
        "# After training, the mean absolute error is : 3.9764989459325397 cells per image.\n",
        "# This error is obtained after training for: 72 epochs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X3Nwa8eyx07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dW2Qiw5yxvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPRxFb5JUdC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}